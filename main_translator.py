import cv2
import mediapipe as mp
import numpy as np
import json
import os
import time

# --- é…ç½® ---
DATA_FILE = "gestures.json"
MATCH_THRESHOLD = 0.25 

class SignLanguageTranslator:
    def __init__(self):
        print("ğŸ“¥ æ­£åœ¨åˆå§‹åŒ– MediaPipe...")
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=1,
            min_detection_confidence=0.7
        )
        self.mp_drawing = mp.solutions.drawing_utils
        self.templates = self.load_templates()

    def load_templates(self):
        if os.path.exists(DATA_FILE):
            try:
                with open(DATA_FILE, 'r') as f:
                    data = json.load(f)
                    return {k: np.array(v) for k, v in data.items()}
            except: pass
        return {}

    def save_templates(self):
        try:
            serializable_data = {k: v.tolist() for k, v in self.templates.items()}
            with open(DATA_FILE, 'w') as f:
                json.dump(serializable_data, f)
            print("ğŸ’¾ æ•°æ®å·²ä¿å­˜")
        except Exception as e:
            print(f"ä¿å­˜å¤±è´¥: {e}")

    def normalize_landmarks(self, landmarks):
        points = np.array([[lm.x, lm.y] for lm in landmarks.landmark])
        center = np.mean(points, axis=0)
        points -= center
        max_dist = np.max(np.linalg.norm(points, axis=1))
        if max_dist > 0: points /= max_dist
        return points.flatten()

    def run(self):
        # å¼ºåˆ¶ä½¿ç”¨ç´¢å¼• 0 (é’ˆå¯¹ä½ çš„æŠ¥é”™ä¿®æ­£)
        print("ğŸ” æ­£åœ¨æ‰“å¼€æ‘„åƒå¤´ (Index 0)...")
        cap = cv2.VideoCapture(0)
        
        # ç»™æ‘„åƒå¤´ä¸€ç‚¹é¢„çƒ­æ—¶é—´
        time.sleep(1.0)

        if not cap.isOpened():
            print("âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´ï¼è¯·æ£€æŸ¥éšç§è®¾ç½®æˆ–æ˜¯å¦è¢«å ç”¨ã€‚")
            return

        print("\n" + "="*40)
        print("ğŸŸ¢ ç¨‹åºå·²å¯åŠ¨ï¼è¯·çœ‹å¼¹å‡ºçš„çª—å£ï¼")
        print("âš ï¸  é‡è¦ï¼šè¯·ç”¨é¼ æ ‡ç‚¹å‡»ä¸€ä¸‹ã€è§†é¢‘ç”»é¢ã€‘")
        print("   ç„¶åæŒ‰é”®ç›˜æ•°å­—é”® [1] [2] [3] å½•åˆ¶")
        print("   æŒ‰ [q] é€€å‡º")
        print("="*40 + "\n")

        while True:
            ret, frame = cap.read()
            
            # ä¿®æ­£ï¼šå¦‚æœè¯»ä¸åˆ°å¸§ï¼Œä¸è¦é€€å‡ºï¼Œè€Œæ˜¯é‡è¯•ï¼ˆé˜²æ­¢å¯åŠ¨æ—¶é»‘å±å¯¼è‡´é—ªé€€ï¼‰
            if not ret:
                print("âš ï¸ ç­‰å¾…æ‘„åƒå¤´ç”»é¢...")
                time.sleep(0.1)
                continue

            frame = cv2.flip(frame, 1)
            img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = self.hands.process(img_rgb)
            
            status_text = "Waiting..."
            color = (200, 200, 200)

            # ç»˜åˆ¶æç¤ºæ–‡å­—ï¼Œæé†’ä½ ç‚¹å‡»çª—å£
            cv2.putText(frame, "CLICK THIS WINDOW FIRST!", (50, 50), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)

            if results.multi_hand_landmarks:
                hand_lms = results.multi_hand_landmarks[0]
                self.mp_drawing.draw_landmarks(frame, hand_lms, self.mp_hands.HAND_CONNECTIONS)
                
                features = self.normalize_landmarks(hand_lms)
                
                min_dist = float('inf')
                best_match = "Unknown"
                
                for name, temp in self.templates.items():
                    dist = np.linalg.norm(features - temp)
                    if dist < min_dist:
                        min_dist = dist
                        best_match = name
                
                if min_dist < MATCH_THRESHOLD:
                    status_text = f"Sign: {best_match}"
                    color = (0, 255, 0)
                else:
                    status_text = "Unknown"
                    color = (0, 0, 255)

                # --- æé€Ÿå½•åˆ¶é€»è¾‘ ---
                key = cv2.waitKey(1) & 0xFF
                
                if key == ord('1'):
                    self.templates["Gesture_1"] = features
                    self.save_templates()
                    print("âœ… åŠ¨ä½œ [1] å·²å½•å…¥ï¼")
                    cv2.circle(frame, (50, 50), 40, (0, 255, 0), -1) # è§†è§‰åé¦ˆ
                
                elif key == ord('2'):
                    self.templates["Gesture_2"] = features
                    self.save_templates()
                    print("âœ… åŠ¨ä½œ [2] å·²å½•å…¥ï¼")
                    cv2.circle(frame, (50, 50), 40, (0, 255, 0), -1)

                elif key == ord('3'):
                    self.templates["Gesture_3"] = features
                    self.save_templates()
                    print("âœ… åŠ¨ä½œ [3] å·²å½•å…¥ï¼")
                    cv2.circle(frame, (50, 50), 40, (0, 255, 0), -1)
                
                elif key == ord('c'):
                    self.templates.clear()
                    self.save_templates()
                    print("ğŸ—‘ï¸ å·²æ¸…ç©º")
            
            # UI æ˜¾ç¤º
            h, w, _ = frame.shape
            cv2.rectangle(frame, (0, h-60), (w, h), (0, 0, 0), -1)
            cv2.putText(frame, status_text, (20, h-20), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)
            
            cv2.imshow('Sign Language (Click Me)', frame)

            # é€€å‡ºé€»è¾‘
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

        cap.release()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    app = SignLanguageTranslator()
    app.run()