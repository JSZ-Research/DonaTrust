<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>AI Sign Language Interpreter</title>
    <script src="https://cdn.socket.io/4.0.0/socket.io.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>
    <style>
        :root { --bg: #0f172a; --accent: #3b82f6; --success: #10b981; }
        body { font-family: system-ui, sans-serif; background: var(--bg); color: #fff; margin: 0; display: flex; height: 100vh; overflow: hidden; }
        .app-container { display: flex; width: 100%; padding: 20px; gap: 20px; }
        .video-section { flex: 2; position: relative; background: #000; border-radius: 20px; overflow: hidden; border: 1px solid #334155; }
        #webcam { width: 100%; height: 100%; object-fit: cover; transform: rotateY(180deg); }
        #output_canvas { position: absolute; top: 0; left: 0; width: 100%; height: 100%; transform: rotateY(180deg); }
        .sidebar { flex: 0.8; display: flex; flex-direction: column; gap: 20px; min-width: 300px; }
        .card { background: rgba(30, 41, 59, 0.7); backdrop-filter: blur(10px); border-radius: 20px; padding: 24px; border: 1px solid #334155; }
        .result-card { text-align: center; flex-grow: 1; display: flex; flex-direction: column; justify-content: center; }
        #status-text { font-size: 48px; font-weight: 800; margin: 20px 0; transition: color 0.3s; }
        .btn-group { display: flex; flex-direction: column; gap: 10px; }
        button { padding: 14px; border-radius: 12px; border: none; font-weight: 600; cursor: pointer; background: var(--accent); color: white; transition: 0.2s; }
        button:hover { opacity: 0.9; transform: translateY(-2px); }
        .dist-label { font-family: monospace; color: #94a3b8; font-size: 14px; }
    </style>
</head>
<body>

<div class="app-container">
    <div class="video-section">
        <video id="webcam" autoplay playsinline></video>
        <canvas id="output_canvas"></canvas>
    </div>

    <div class="sidebar">
        <div class="card result-card">
            <h3 style="color: #94a3b8; text-transform: uppercase; font-size: 14px;">Live Translation</h3>
            <div id="status-text">Standby</div>
            <div id="dist-info" class="dist-label">Dist: --</div>
        </div>

        <div class="card">
            <h4>Training Actions</h4>
            <div class="btn-group">
                <button onclick="record('Hello')">Record "Hello"</button>
                <button onclick="record('Thank You')">Record "Thank You"</button>
                <button style="background: #475569;" onclick="record('Goodbye')">Record "Goodbye"</button>
            </div>
            <p style="font-size: 12px; color: #64748b; margin-top: 15px;">Hold your hand steady and click to save the gesture.</p>
        </div>
    </div>
</div>

<script>
    const socket = io();
    const videoElement = document.getElementById('webcam');
    const canvasElement = document.getElementById('output_canvas');
    const canvasCtx = canvasElement.getContext('2d');
    let lastFeatures = null;

    // --- Critical: Unified Normalization Logic ---
    function normalize(landmarks) {
        // 1. Get raw coordinates
        const points = landmarks.map(l => ({x: l.x, y: l.y}));
        
        // 2. Calculate Center (Mean)
        const centerX = points.reduce((s, p) => s + p.x, 0) / 21;
        const centerY = points.reduce((s, p) => s + p.y, 0) / 21;
        
        // 3. Center and find Max Distance for scaling
        let maxDist = 0;
        const centered = points.map(p => {
            const dx = p.x - centerX;
            const dy = p.y - centerY;
            const d = Math.sqrt(dx*dx + dy*dy);
            if (d > maxDist) maxDist = d;
            return {dx, dy};
        });
        
        // 4. Flatten to [x0, y0, x1, y1...] scaled by maxDist
        const features = [];
        centered.forEach(p => {
            features.push(p.dx / (maxDist || 1));
            features.push(p.dy / (maxDist || 1));
        });
        return features;
    }

    function onResults(results) {
        // Dynamic canvas sizing
        if (canvasElement.width !== videoElement.videoWidth) {
            canvasElement.width = videoElement.videoWidth;
            canvasElement.height = videoElement.videoHeight;
        }

        canvasCtx.save();
        canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
        
        if (results.multiHandLandmarks && results.multiHandLandmarks.length > 0) {
            const lms = results.multiHandLandmarks[0];
            drawConnectors(canvasCtx, lms, HAND_CONNECTIONS, {color: '#3b82f6', lineWidth: 4});
            drawLandmarks(canvasCtx, lms, {color: '#ffffff', lineWidth: 1, radius: 2});
            
            lastFeatures = normalize(lms);
            socket.emit('process_frame', {features: lastFeatures});
        } else {
            document.getElementById('status-text').innerText = "No Hand";
            document.getElementById('status-text').style.color = "#64748b";
        }
        canvasCtx.restore();
    }

    const hands = new Hands({locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`});
    hands.setOptions({maxNumHands: 1, modelComplexity: 1, minDetectionConfidence: 0.7});
    hands.onResults(onResults);

    const camera = new Camera(videoElement, {
        onFrame: async () => { await hands.send({image: videoElement}); },
        width: 1280, height: 720
    });
    camera.start();

    // Listen for Backend Matching
    socket.on('response_result', (data) => {
        const textElem = document.getElementById('status-text');
        textElem.innerText = data.best_match;
        textElem.style.color = data.best_match === "Unknown" ? "#ef4444" : "#10b981";
        document.getElementById('dist-info').innerText = "Confidence Dist: " + data.dist;
    });

    function record(name) {
        if (lastFeatures) {
            socket.emit('record_gesture', {name: name, features: lastFeatures});
            alert("Recorded: " + name);
        } else {
            alert("Show your hand first!");
        }
    }
</script>
</body>
</html>
